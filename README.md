# LoT
Language of Thought Modelling of Cognitive Algorithms

This project builds models connecting implicit generative strategies to explicit reasoning, contributing to AI alignment, cognitive modeling, and understanding abstraction in diverse human, animal, and machine intelligence.

This project was driven by two research interns:
1. Soham Kulkarni @kulkarnisoham833
2. Ridhi Bandaru @bendemonium


In this project, we model how human children and adults, crows, monkeys and AI (simple recurrent networks) generate hierarchical structures and explanations varying in ecological validity: (1) an artificial language learning task probing inductive biases as well as learning capacity under controlled lab conditions, (2) a Russian nesting doll completion task, and (3) a hierarchical drawing task replicating real-world nested visual structures. We encode sequential behaviors as symbolic strings and use information-theoretic metrics such as string similarity, Kolmogorov complexity / Minimum Description Length to quantify generative ability. Bayesian models, including Dirichlet mixture models, are used to infer latent cognitive algorithms—linear vs. hierarchical—underlying structure generation. In addition, we are implementing these cognitive algorithms in a language-of-thought framework to formalize complexity benchmarks across various tasks. 

We are tracing Chain of Thought (CoT) reasoning—the unfolding of intermediate mental steps—by analyzing human and LLM-generated explanations using natural language processing methods—semantic parsing, syntactic complexity analysis, and embedding-based similarity models. We are also incorporating program synthesis and induction techniques to infer the cognitive programs underlying both behavior and explanation. Finally, we model how information capacity constraints shape these cognitive algorithms by using various measures of visuospatial working memory.
