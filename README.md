# LoT
Language of Thought Modelling of Cognitive Algorithms

This project builds models connecting implicit generative strategies to explicit reasoning, contributing to AI alignment, cognitive modeling, and understanding abstraction in diverse human, animal, and machine intelligence.

The following interns contributed to this project:
1. Soham Kulkarni @kulkarnisoham833
2. Ridhi Bandaru @bendemonium


In this project, we model how human children and adults, crows, monkeys and AI (simple recurrent networks) generate hierarchical structures and explanations varying in ecological validity: (1) an artificial language learning task probing inductive biases as well as learning capacity under controlled lab conditions, (2) a Russian nesting doll completion task, and (3) a hierarchical drawing task replicating real-world nested visual structures. We encode sequential behaviors as symbolic strings and use information-theoretic metrics such as string similarity, Kolmogorov complexity / Minimum Description Length to quantify generative ability. Bayesian models, including Dirichlet mixture models, are used to infer latent cognitive algorithms—linear vs. hierarchical—underlying structure generation. In addition, we are implementing these cognitive algorithms in a language-of-thought framework to formalize complexity benchmarks across various tasks. Finally, we model how information capacity constraints shape these cognitive algorithms by using various measures of visuospatial working memory.
